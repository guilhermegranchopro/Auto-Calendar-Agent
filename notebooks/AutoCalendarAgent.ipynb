{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† EY AI Challenge - Deadline Manager Agent (Google Colab)\n",
    "\n",
    "**Modular notebook for Google Colab submission:** OCR, date parsing, working-days, AI agent for Portuguese legal deadlines with calendar integration.\n",
    "\n",
    "**Google Drive Setup:** This notebook is designed to run in Google Colab with files stored in Google Drive under \"EY Challenge\" folder.\n",
    "\n",
    "**Folder Structure:**\n",
    "- `/content/drive/MyDrive/EY Challenge/` - Main project folder\n",
    "- `/content/drive/MyDrive/EY Challenge/data/` - Input documents folder\n",
    "- `/content/drive/MyDrive/EY Challenge/AutoCalendarAgent.ipynb` - This notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOGLE COLAB SETUP: Mount Google Drive and install dependencies\n",
    "print(\"üîó Setting up Google Colab environment...\")\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"‚úÖ Google Drive mounted successfully!\")\n",
    "\n",
    "# Change to project directory\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive/EY Challenge')\n",
    "print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# DEPENDENCIES: Installing required packages for AI Deadline Manager\n",
    "print(\"üì¶ Installing system dependencies...\")\n",
    "!apt-get update && apt-get install -y tesseract-ocr\n",
    "\n",
    "print(\"üêç Installing Python packages...\")\n",
    "!pip install --upgrade pip\n",
    "!pip install --quiet pytesseract PyPDF2 pillow dateparser python-dateutil holidays transformers\n",
    "!pip install --quiet matplotlib pandas plotly\n",
    "!pip install --quiet python-docx  # For DOCX file processing\n",
    "\n",
    "# Install AI model dependencies\n",
    "print(\"ü§ñ Installing AI model dependencies...\")\n",
    "!pip install --quiet google-generativeai langchain-google-genai langchain-core\n",
    "\n",
    "print(\"‚úÖ All dependencies installed successfully!\")\n",
    "print(f\"üìç Current working directory: {os.getcwd()}\")\n",
    "print(f\"üìÇ Data folder exists: {os.path.exists('data')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS: Essential libraries for AI Deadline Manager\n",
    "import warnings\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Literal, Optional\n",
    "\n",
    "# Data processing libraries\n",
    "import holidays\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "from dateparser.search import search_dates\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from PIL import Image\n",
    "from PyPDF2 import PdfReader\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# AI Model imports\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Google Colab specific imports\n",
    "from google.colab import files\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "# Configure matplotlib for Colab\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Verify environment\n",
    "print(\"‚úÖ All imports loaded successfully!\")\n",
    "print(f\"üìç Working directory: {os.getcwd()}\")\n",
    "print(f\"üìÇ Data folder path: {Path('data').absolute()}\")\n",
    "print(f\"üìÇ Data folder exists: {Path('data').exists()}\")\n",
    "\n",
    "# List data files if folder exists\n",
    "if Path('data').exists():\n",
    "    data_files = list(Path('data').iterdir())\n",
    "    print(f\"üìÑ Found {len(data_files)} files in data folder:\")\n",
    "    for i, file in enumerate(data_files[:10], 1):  # Show first 10 files\n",
    "        print(f\"   {i}. {file.name}\")\n",
    "    if len(data_files) > 10:\n",
    "        print(f\"   ... and {len(data_files) - 10} more files\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Data folder not found. Please ensure 'data' folder exists in your Google Drive EY Challenge folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñ AI Model Configuration\n",
    "Configure and select between different AI models for deadline extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI MODEL CONFIGURATION\n",
    "# Configure your Gemini API key\n",
    "GEMINI_API_KEY = \"AIzaSyB1XJV_CWEu9zojtETnViNEhwoFa8CF-FE\"  # Replace with your API key\n",
    "\n",
    "# Configure Google Generative AI\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Available AI models\n",
    "AVAILABLE_MODELS = [\"gemini-pro\", \"gemini-2.0-flash-001\"]\n",
    "\n",
    "# Model selection (change this to switch models)\n",
    "SELECTED_AI_MODEL = \"gemini-2.0-flash-001\"  # Change to \"gemini-pro\" if preferred\n",
    "\n",
    "print(f\"ü§ñ AI Model Configuration:\")\n",
    "print(f\"   ‚Ä¢ Available models: {AVAILABLE_MODELS}\")\n",
    "print(f\"   ‚Ä¢ Selected model: {SELECTED_AI_MODEL}\")\n",
    "print(f\"   ‚Ä¢ API configured: {'‚úÖ' if GEMINI_API_KEY else '‚ùå'}\")\n",
    "\n",
    "# Initialize the selected model\n",
    "if SELECTED_AI_MODEL == \"gemini-2.0-flash-001\":\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash-001\",\n",
    "        google_api_key=GEMINI_API_KEY,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    print(f\"   ‚Ä¢ Using LangChain ChatGoogleGenerativeAI with {SELECTED_AI_MODEL}\")\n",
    "else:\n",
    "    genai_model = genai.GenerativeModel(\"gemini-pro\")\n",
    "    print(f\"   ‚Ä¢ Using Google GenerativeAI with {SELECTED_AI_MODEL}\")\n",
    "\n",
    "print(\"‚úÖ AI model initialization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üñºÔ∏è OCR & PDF Extraction\n",
    "Functions to read text in images (Tesseract) and PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_image(path):\n",
    "    \"\"\"Enhanced extraction of text from image with error handling for Google Colab.\"\"\"\n",
    "    try:\n",
    "        image = Image.open(path)\n",
    "        # Try multiple languages for better OCR results\n",
    "        text = pytesseract.image_to_string(image, lang=\"por+eng\")\n",
    "        print(f\"   üì∑ OCR extracted {len(text)} characters from {Path(path).name}\")\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing image {path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(path):\n",
    "    \"\"\"Enhanced extraction of text from PDF with better error handling for Google Colab.\"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(path)\n",
    "        text_parts = []\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text_parts.append(page_text)\n",
    "        \n",
    "        full_text = \"\\n\".join(text_parts)\n",
    "        print(f\"   üìÑ PDF extracted {len(full_text)} characters from {len(reader.pages)} pages in {Path(path).name}\")\n",
    "        return full_text\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing PDF {path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_text_from_docx(path):\n",
    "    \"\"\"Extract text from Word documents using python-docx.\"\"\"\n",
    "    try:\n",
    "        from docx import Document\n",
    "        doc = Document(path)\n",
    "        text_parts = []\n",
    "        \n",
    "        for paragraph in doc.paragraphs:\n",
    "            if paragraph.text.strip():\n",
    "                text_parts.append(paragraph.text)\n",
    "        \n",
    "        full_text = \"\\n\".join(text_parts)\n",
    "        print(f\"   üìù DOCX extracted {len(full_text)} characters from {Path(path).name}\")\n",
    "        return full_text\n",
    "    except ImportError:\n",
    "        print(f\"‚ö†Ô∏è python-docx not available. Installing...\")\n",
    "        !pip install --quiet python-docx\n",
    "        # Retry import\n",
    "        from docx import Document\n",
    "        doc = Document(path)\n",
    "        text_parts = [paragraph.text for paragraph in doc.paragraphs if paragraph.text.strip()]\n",
    "        full_text = \"\\n\".join(text_parts)\n",
    "        print(f\"   üìù DOCX extracted {len(full_text)} characters from {Path(path).name}\")\n",
    "        return full_text\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing DOCX {path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_text_from_txt(path):\n",
    "    \"\"\"Extract text from plain text files with encoding detection.\"\"\"\n",
    "    try:\n",
    "        # Try different encodings\n",
    "        encodings = ['utf-8', 'latin-1', 'cp1252']\n",
    "        \n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                with open(path, 'r', encoding=encoding) as f:\n",
    "                    text = f.read()\n",
    "                print(f\"   üìÑ TXT extracted {len(text)} characters from {Path(path).name} (encoding: {encoding})\")\n",
    "                return text.strip()\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "        \n",
    "        print(f\"‚ùå Could not decode text file {path} with any encoding\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing TXT {path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "print(\"‚úÖ File processing functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Data extraction (NLU)\n",
    "Extract the first future date from a free text like `dateparser.search.search_dates`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced date inference and working days calculation\n",
    "import re\n",
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "\n",
    "def add_working_days(start_date, num_days):\n",
    "    \"\"\"Add working days to a date, skipping weekends and Portuguese holidays\"\"\"\n",
    "    pt_hols = holidays.Portugal()\n",
    "    current_date = start_date\n",
    "    days_added = 0\n",
    "\n",
    "    while days_added < num_days:\n",
    "        current_date += timedelta(days=1)\n",
    "        if current_date.weekday() < 5 and current_date not in pt_hols:\n",
    "            days_added += 1\n",
    "\n",
    "    return current_date\n",
    "\n",
    "\n",
    "def apply_portuguese_tax_rules(text, reference_date=None):\n",
    "    \"\"\"Apply specific Portuguese tax deadline rules\"\"\"\n",
    "    ref = reference_date or datetime.now()\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Modelo 22 (IRS) - due by July 31st\n",
    "    if \"modelo 22\" in text_lower or (\"irs\" in text_lower and \"modelo\" in text_lower):\n",
    "        deadline = datetime(ref.year, 7, 31)\n",
    "        if deadline < ref:\n",
    "            deadline = datetime(ref.year + 1, 7, 31)\n",
    "        return {\"deadline\": deadline, \"rule\": \"Modelo 22 - IRS deadline\"}\n",
    "\n",
    "    # IES - due by April 15th\n",
    "    if \"ies\" in text_lower:\n",
    "        deadline = datetime(ref.year, 4, 15)\n",
    "        if deadline < ref:\n",
    "            deadline = datetime(ref.year + 1, 4, 15)\n",
    "        return {\"deadline\": deadline, \"rule\": \"IES deadline\"}\n",
    "\n",
    "    # Modelo 30 (Reten√ß√µes na fonte) - monthly, 20th of following month\n",
    "    if (\n",
    "        \"modelo 30\" in text_lower\n",
    "        or \"reten√ß√µes na fonte\" in text_lower\n",
    "        or \"retencao na fonte\" in text_lower\n",
    "    ):\n",
    "        next_month = ref.replace(day=1) + relativedelta(months=1)\n",
    "        deadline = next_month.replace(day=20)\n",
    "        return {\"deadline\": deadline, \"rule\": \"Modelo 30 - Monthly retention deadline\"}\n",
    "\n",
    "    # IVA declarations - quarterly deadlines\n",
    "    if \"iva\" in text_lower and (\n",
    "        \"declaracao\" in text_lower or \"declara√ß√£o\" in text_lower\n",
    "    ):\n",
    "        # Find next quarterly deadline\n",
    "        quarters = [(3, 31), (6, 30), (9, 30), (12, 31)]\n",
    "        for month, day in quarters:\n",
    "            deadline = datetime(ref.year, month, day)\n",
    "            if deadline > ref:\n",
    "                return {\"deadline\": deadline, \"rule\": \"IVA quarterly declaration\"}\n",
    "        # If all quarters passed, use first quarter of next year\n",
    "        deadline = datetime(ref.year + 1, 3, 31)\n",
    "        return {\"deadline\": deadline, \"rule\": \"IVA quarterly declaration\"}\n",
    "\n",
    "    # SAF-T - monthly, 25th of following month\n",
    "    if \"saf-t\" in text_lower:\n",
    "        next_month = ref.replace(day=1) + relativedelta(months=1)\n",
    "        deadline = next_month.replace(day=25)\n",
    "        return {\"deadline\": deadline, \"rule\": \"SAF-T monthly deadline\"}\n",
    "\n",
    "    # DMR (Declara√ß√£o Mensal de Remunera√ß√µes) - 10th of following month\n",
    "    if \"dmr\" in text_lower or \"declara√ß√£o mensal de remunera√ß√µes\" in text_lower:\n",
    "        next_month = ref.replace(day=1) + relativedelta(months=1)\n",
    "        deadline = next_month.replace(day=10)\n",
    "        return {\"deadline\": deadline, \"rule\": \"DMR monthly deadline\"}\n",
    "\n",
    "    # Working days patterns\n",
    "    # \"30 dias √∫teis\"\n",
    "    working_days_pattern = r\"(\\d+)\\s+dias?\\s+√∫teis\"\n",
    "    match = re.search(working_days_pattern, text_lower)\n",
    "    if match:\n",
    "        days = int(match.group(1))\n",
    "        deadline = add_working_days(ref, days)\n",
    "        return {\"deadline\": deadline, \"rule\": f\"{days} working days from notification\"}\n",
    "\n",
    "    # \"prazo de X dias\"\n",
    "    days_pattern = r\"prazo\\s+(?:de\\s+)?(\\d+)\\s+dias?\"\n",
    "    match = re.search(days_pattern, text_lower)\n",
    "    if match:\n",
    "        days = int(match.group(1))\n",
    "        deadline = ref + timedelta(days=days)\n",
    "        return {\"deadline\": deadline, \"rule\": f\"{days} days from notification\"}\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def search_dates(text, languages=None, settings=None):\n",
    "    \"\"\"Busca por datas em um texto, tentando inferir o m√°ximo poss√≠vel de formatos.\"\"\"\n",
    "    # Tenta fazer o parsing direto\n",
    "    try:\n",
    "        return [(text, parse(text, languages=languages))]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Se falhar, tenta com configura√ß√µes\n",
    "    if settings:\n",
    "        settings.get(\"PREFER_DATES_FROM\", \"future\") == \"future\"\n",
    "        settings.get(\"RELATIVE_BASE\", None)\n",
    "        settings.get(\"DATE_ORDER\", \"DMY\")\n",
    "\n",
    "        # Tenta identificar a data com base nas configura√ß√µes\n",
    "        try:\n",
    "            return [(text, parse(text, languages=languages, settings=settings))]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return []\n",
    "\n",
    "\n",
    "def infer_deadline(text, base_date=None):\n",
    "    \"\"\"Enhanced deadline identification with Portuguese legal patterns.\"\"\"\n",
    "    base = base_date or datetime.now()\n",
    "\n",
    "    # Try rule-based approach first\n",
    "    rule_result = apply_portuguese_tax_rules(text, base)\n",
    "    if rule_result:\n",
    "        return rule_result[\"deadline\"]\n",
    "\n",
    "    # First try with dateparser\n",
    "    res = search_dates(\n",
    "        text,\n",
    "        languages=[\"pt\", \"en\"],\n",
    "        settings={\n",
    "            \"PREFER_DATES_FROM\": \"future\",\n",
    "            \"RELATIVE_BASE\": base,\n",
    "            \"DATE_ORDER\": \"DMY\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if res:\n",
    "        return res[0][1]\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÖ Work days calculation (PT)\n",
    "Add work days to a date, excluding weekends and Portuguese holidays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_working_days(start_date, days):\n",
    "    \"\"\"Base de un√ß√£o auxiliar para somar dias √∫teis a uma data, gerir f√©rias judiciais, etc.\"\"\"\n",
    "    pt_hols = holidays.Portugal()\n",
    "    curr = start_date\n",
    "    added = 0\n",
    "    while added < days:\n",
    "        curr += relativedelta(days=1)\n",
    "        if curr.weekday() < 5 and curr not in pt_hols:\n",
    "            added += 1\n",
    "    return curr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñ Deadline Agent (LLM Free)\n",
    "One type of open-source model (Flan-T5 small) to apply the following rules:\n",
    "- Modelo 22: up to 31/jul\n",
    "- IES: 15/apr (current and next year)\n",
    "- Others: infer via NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced implementation using Flan-T5 with Portuguese tax rules\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "\n",
    "def llm_generate(prompt: str, max_length: int = 256) -> str:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    outs = model.generate(\n",
    "        inputs, num_beams=4, early_stopping=True, max_length=max_length\n",
    "    )\n",
    "    return tokenizer.decode(outs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def apply_portuguese_tax_rules(text, reference_date=None):\n",
    "    \"\"\"Apply specific Portuguese tax deadline rules.\"\"\"\n",
    "    ref = reference_date or datetime.now()\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Modelo 22 (IRS) - due by July 31st\n",
    "    if \"modelo 22\" in text_lower or \"irs\" in text_lower:\n",
    "        deadline = datetime(ref.year, 7, 31)\n",
    "        if deadline < ref:\n",
    "            deadline = datetime(ref.year + 1, 7, 31)\n",
    "        return {\"deadline\": deadline, \"rule\": \"Modelo 22 - IRS deadline\"}\n",
    "\n",
    "    # IES - due by April 15th\n",
    "    if \"ies\" in text_lower:\n",
    "        deadline = datetime(ref.year, 4, 15)\n",
    "        if deadline < ref:\n",
    "            deadline = datetime(ref.year + 1, 4, 15)\n",
    "        return {\"deadline\": deadline, \"rule\": \"IES deadline\"}\n",
    "\n",
    "    # Modelo 30 (Reten√ß√µes na fonte) - monthly, 20th of following month\n",
    "    if (\n",
    "        \"modelo 30\" in text_lower\n",
    "        or \"reten√ß√µes na fonte\" in text_lower\n",
    "        or \"retencao na fonte\" in text_lower\n",
    "    ):\n",
    "        next_month = ref.replace(day=1) + relativedelta(months=1)\n",
    "        deadline = next_month.replace(day=20)\n",
    "        return {\"deadline\": deadline, \"rule\": \"Modelo 30 - Monthly retention deadline\"}\n",
    "\n",
    "    # IVA declarations - quarterly deadlines\n",
    "    if \"iva\" in text_lower and \"declaracao\" in text_lower:\n",
    "        # Find next quarterly deadline\n",
    "        quarters = [(3, 31), (6, 30), (9, 30), (12, 31)]\n",
    "        for month, day in quarters:\n",
    "            deadline = datetime(ref.year, month, day)\n",
    "            if deadline > ref:\n",
    "                return {\"deadline\": deadline, \"rule\": \"IVA quarterly declaration\"}\n",
    "        # If all quarters passed, use first quarter of next year\n",
    "        deadline = datetime(ref.year + 1, 3, 31)\n",
    "        return {\"deadline\": deadline, \"rule\": \"IVA quarterly declaration\"}\n",
    "\n",
    "    # SAF-T - monthly, 25th of following month\n",
    "    if \"saf-t\" in text_lower:\n",
    "        next_month = ref.replace(day=1) + relativedelta(months=1)\n",
    "        deadline = next_month.replace(day=25)\n",
    "        return {\"deadline\": deadline, \"rule\": \"SAF-T monthly deadline\"}\n",
    "\n",
    "    # DMR (Declara√ß√£o Mensal de Remunera√ß√µes) - 10th of following month\n",
    "    if \"dmr\" in text_lower or \"declara√ß√£o mensal de remunera√ß√µes\" in text_lower:\n",
    "        next_month = ref.replace(day=1) + relativedelta(months=1)\n",
    "        deadline = next_month.replace(day=10)\n",
    "        return {\"deadline\": deadline, \"rule\": \"DMR monthly deadline\"}\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def agent_process(text, reference_date=None):\n",
    "    \"\"\"Enhanced agent that applies Portuguese tax rules and LLM processing.\"\"\"\n",
    "    ref = reference_date or datetime.now()\n",
    "\n",
    "    # First try rule-based approach\n",
    "    rule_result = apply_portuguese_tax_rules(text, ref)\n",
    "    if rule_result:\n",
    "        return rule_result\n",
    "\n",
    "    # Try deadline inference from text\n",
    "    inferred_date = infer_deadline(text, ref)\n",
    "    if inferred_date:\n",
    "        return {\"deadline\": inferred_date, \"rule\": \"Natural language inference\"}\n",
    "\n",
    "    # Fall back to LLM\n",
    "    prompt = f\"\"\"\n",
    "You are a Portuguese tax deadline assistant. Analyze this text and determine the deadline.\n",
    "Reference date: {ref.strftime(\"%Y-%m-%d\")}\n",
    "Text: \"{text}\"\n",
    "\n",
    "Return a JSON object with 'deadline' (YYYY-MM-DD format) and 'reasoning'.\n",
    "If no deadline can be determined, return {{'error': 'No deadline found'}}.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        raw = llm_generate(prompt)\n",
    "        # Try to parse LLM response\n",
    "        if \"deadline\" in raw.lower():\n",
    "            # Extract date from response\n",
    "            import re\n",
    "\n",
    "            date_pattern = r\"(\\d{4}-\\d{2}-\\d{2})\"\n",
    "            match = re.search(date_pattern, raw)\n",
    "            if match:\n",
    "                deadline = datetime.strptime(match.group(1), \"%Y-%m-%d\")\n",
    "                return {\"deadline\": deadline, \"rule\": \"LLM inference\"}\n",
    "\n",
    "        return {\"error\": f\"Could not parse deadline from: {raw}\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"LLM processing error: {e}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation using Gemini LLM\n",
    "\n",
    "\n",
    "def config_llm_gemini(temperature: int):\n",
    "    \"\"\"LLM api calling using Gemini\"\"\"\n",
    "    # Steps for students:\n",
    "    # - Go to https://aistudio.google.com/app/apikey and generate your Gemini API key.\n",
    "    # - Add the necessary packages to your requirements.txt:\n",
    "    #    langchain\n",
    "    #    langchain-google-genai\n",
    "    # - Run the following command to install them:\n",
    "    #     !pip install -r requirements.txt\n",
    "    # - Follow the official integration guide for LangChain + Google Generative AI:\n",
    "    #     https://python.langchain.com/docs/integrations/chat/google_generative_ai/\n",
    "    # Pay attention to the request limits of the chosen model.\n",
    "    return \"llm\"  # Should return the LLM response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîó Calendar integration (Optional)\n",
    "Function to create events in external calendar tool\n",
    "\n",
    "# ENHANCED AI DEADLINE AGENT with Multi-Model Support for Google Colab\n",
    "\n",
    "def process_with_gemini_ai(text: str, reference_date=None, ai_model: str = SELECTED_AI_MODEL) -> dict:\n",
    "    \"\"\"Process text using the selected Gemini AI model in Google Colab environment.\"\"\"\n",
    "    ref_date = reference_date or datetime.now()\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are a Portuguese legal deadline extraction specialist. Analyze the following text and extract deadline information.\n",
    "\n",
    "Reference date: {ref_date.strftime('%Y-%m-%d')}\n",
    "Text to analyze: {text}\n",
    "\n",
    "Please identify:\n",
    "1. Any specific deadlines mentioned\n",
    "2. The type of legal obligation (Modelo 22, IES, IVA, etc.)\n",
    "3. Calculate the exact deadline date\n",
    "4. Provide reasoning for your conclusion\n",
    "\n",
    "Return a JSON object with:\n",
    "- \"deadline\": \"YYYY-MM-DD\" (if found)\n",
    "- \"obligation_type\": \"description\"\n",
    "- \"reasoning\": \"explanation\"\n",
    "- \"confidence\": \"high/medium/low\"\n",
    "\n",
    "If no deadline is found, return {{\"error\": \"No deadline identified\"}}.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        if ai_model == \"gemini-2.0-flash-001\":\n",
    "            # Use LangChain ChatGoogleGenerativeAI\n",
    "            response = llm.invoke(prompt)\n",
    "            response_text = response.content\n",
    "        else:\n",
    "            # Use direct Google GenerativeAI\n",
    "            response = genai_model.generate_content(prompt)\n",
    "            response_text = response.text\n",
    "            \n",
    "        return {\"response\": response_text, \"model_used\": ai_model}\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"AI processing failed: {str(e)}\", \"model_used\": ai_model}\n",
    "\n",
    "\n",
    "def apply_portuguese_tax_rules(text, reference_date=None):\n",
    "    \"\"\"Apply specific Portuguese tax deadline rules.\"\"\"\n",
    "    ref = reference_date or datetime.now()\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Modelo 22 (IRS) - due by July 31st\n",
    "    if \"modelo 22\" in text_lower or (\"irs\" in text_lower and \"modelo\" in text_lower):\n",
    "        deadline = datetime(ref.year, 7, 31)\n",
    "        if deadline < ref:\n",
    "            deadline = datetime(ref.year + 1, 7, 31)\n",
    "        return {\"deadline\": deadline, \"rule\": \"Modelo 22 - IRS deadline\"}\n",
    "\n",
    "    # IES - due by April 15th\n",
    "    if \"ies\" in text_lower:\n",
    "        deadline = datetime(ref.year, 4, 15)\n",
    "        if deadline < ref:\n",
    "            deadline = datetime(ref.year + 1, 4, 15)\n",
    "        return {\"deadline\": deadline, \"rule\": \"IES deadline\"}\n",
    "\n",
    "    # Modelo 30 (Reten√ß√µes na fonte) - monthly, 20th of following month\n",
    "    if (\n",
    "        \"modelo 30\" in text_lower\n",
    "        or \"reten√ß√µes na fonte\" in text_lower\n",
    "        or \"retencao na fonte\" in text_lower\n",
    "    ):\n",
    "        next_month = ref.replace(day=1) + relativedelta(months=1)\n",
    "        deadline = next_month.replace(day=20)\n",
    "        return {\"deadline\": deadline, \"rule\": \"Modelo 30 - Monthly retention deadline\"}\n",
    "\n",
    "    # IVA declarations - quarterly deadlines\n",
    "    if \"iva\" in text_lower and (\n",
    "        \"declaracao\" in text_lower or \"declara√ß√£o\" in text_lower\n",
    "    ):\n",
    "        # Find next quarterly deadline\n",
    "        quarters = [(3, 31), (6, 30), (9, 30), (12, 31)]\n",
    "        for month, day in quarters:\n",
    "            deadline = datetime(ref.year, month, day)\n",
    "            if deadline > ref:\n",
    "                return {\"deadline\": deadline, \"rule\": \"IVA quarterly declaration\"}\n",
    "        # If all quarters passed, use first quarter of next year\n",
    "        deadline = datetime(ref.year + 1, 3, 31)\n",
    "        return {\"deadline\": deadline, \"rule\": \"IVA quarterly declaration\"}\n",
    "\n",
    "    # SAF-T - monthly, 25th of following month\n",
    "    if \"saf-t\" in text_lower:\n",
    "        next_month = ref.replace(day=1) + relativedelta(months=1)\n",
    "        deadline = next_month.replace(day=25)\n",
    "        return {\"deadline\": deadline, \"rule\": \"SAF-T monthly deadline\"}\n",
    "\n",
    "    # DMR (Declara√ß√£o Mensal de Remunera√ß√µes) - 10th of following month\n",
    "    if \"dmr\" in text_lower or \"declara√ß√£o mensal de remunera√ß√µes\" in text_lower:\n",
    "        next_month = ref.replace(day=1) + relativedelta(months=1)\n",
    "        deadline = next_month.replace(day=10)\n",
    "        return {\"deadline\": deadline, \"rule\": \"DMR monthly deadline\"}\n",
    "\n",
    "    # Working days patterns\n",
    "    import re\n",
    "    working_days_pattern = r\"(\\d+)\\s+dias?\\s+√∫teis\"\n",
    "    match = re.search(working_days_pattern, text_lower)\n",
    "    if match:\n",
    "        days = int(match.group(1))\n",
    "        deadline = add_working_days(ref, days)\n",
    "        return {\"deadline\": deadline, \"rule\": f\"{days} working days from notification\"}\n",
    "\n",
    "    # \"prazo de X dias\"\n",
    "    days_pattern = r\"prazo\\s+(?:de\\s+)?(\\d+)\\s+dias?\"\n",
    "    match = re.search(days_pattern, text_lower)\n",
    "    if match:\n",
    "        days = int(match.group(1))\n",
    "        deadline = ref + timedelta(days=days)\n",
    "        return {\"deadline\": deadline, \"rule\": f\"{days} days from notification\"}\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def agent_process(text, reference_date=None, ai_model: str = None):\n",
    "    \"\"\"Enhanced agent that applies Portuguese tax rules and AI processing.\"\"\"\n",
    "    if ai_model is None:\n",
    "        ai_model = SELECTED_AI_MODEL\n",
    "        \n",
    "    ref = reference_date or datetime.now()\n",
    "    \n",
    "    print(f\"ü§ñ Processing with AI model: {ai_model}\")\n",
    "\n",
    "    # First try rule-based approach\n",
    "    rule_result = apply_portuguese_tax_rules(text, ref)\n",
    "    if rule_result:\n",
    "        rule_result[\"processing_method\"] = \"Rule-based\"\n",
    "        rule_result[\"ai_model_used\"] = ai_model\n",
    "        return rule_result\n",
    "\n",
    "    # Try deadline inference from text\n",
    "    inferred_date = infer_deadline(text, ref)\n",
    "    if inferred_date:\n",
    "        return {\n",
    "            \"deadline\": inferred_date, \n",
    "            \"rule\": \"Natural language inference\",\n",
    "            \"processing_method\": \"Date parsing\",\n",
    "            \"ai_model_used\": ai_model\n",
    "        }\n",
    "\n",
    "    # Fall back to AI processing\n",
    "    ai_result = process_with_gemini_ai(text, ref, ai_model)\n",
    "    \n",
    "    if \"error\" not in ai_result:\n",
    "        # Try to parse AI response\n",
    "        import re\n",
    "        import json\n",
    "        \n",
    "        response_text = ai_result[\"response\"]\n",
    "        \n",
    "        # Try to extract JSON from response\n",
    "        try:\n",
    "            # Look for JSON in the response\n",
    "            json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_data = json.loads(json_match.group())\n",
    "                if \"deadline\" in json_data and json_data[\"deadline\"] != \"No deadline identified\":\n",
    "                    deadline = datetime.strptime(json_data[\"deadline\"], \"%Y-%m-%d\")\n",
    "                    return {\n",
    "                        \"deadline\": deadline,\n",
    "                        \"rule\": f\"AI inference: {json_data.get('obligation_type', 'Unknown')}\",\n",
    "                        \"reasoning\": json_data.get('reasoning', 'AI analysis'),\n",
    "                        \"confidence\": json_data.get('confidence', 'medium'),\n",
    "                        \"processing_method\": \"AI analysis\",\n",
    "                        \"ai_model_used\": ai_model\n",
    "                    }\n",
    "        except (json.JSONDecodeError, ValueError, KeyError):\n",
    "            pass\n",
    "            \n",
    "        # Try to extract date patterns from response\n",
    "        date_pattern = r\"(\\d{4}-\\d{2}-\\d{2})\"\n",
    "        match = re.search(date_pattern, response_text)\n",
    "        if match:\n",
    "            try:\n",
    "                deadline = datetime.strptime(match.group(1), \"%Y-%m-%d\")\n",
    "                return {\n",
    "                    \"deadline\": deadline,\n",
    "                    \"rule\": \"AI pattern extraction\",\n",
    "                    \"processing_method\": \"AI analysis\",\n",
    "                    \"ai_model_used\": ai_model\n",
    "                }\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    return {\n",
    "        \"error\": f\"Could not extract deadline. AI response: {ai_result.get('response', ai_result.get('error', 'Unknown error'))}\",\n",
    "        \"processing_method\": \"Failed\",\n",
    "        \"ai_model_used\": ai_model\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Enhanced AI Deadline Agent with multi-model support ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_calendar_event(summary, start, end, timezone='UTC'):\n",
    "#     pass  # implementar conforme API desejada\n",
    "\n",
    "# MODEL SWITCHING FUNCTIONALITY for Google Colab\n",
    "\n",
    "def switch_ai_model(model_name: str):\n",
    "    \"\"\"Switch between available AI models in Google Colab environment.\"\"\"\n",
    "    global SELECTED_AI_MODEL, llm, genai_model\n",
    "    \n",
    "    if model_name not in AVAILABLE_MODELS:\n",
    "        print(f\"‚ùå Invalid model. Available models: {AVAILABLE_MODELS}\")\n",
    "        return False\n",
    "    \n",
    "    SELECTED_AI_MODEL = model_name\n",
    "    \n",
    "    try:\n",
    "        if model_name == \"gemini-2.0-flash-001\":\n",
    "            llm = ChatGoogleGenerativeAI(\n",
    "                model=\"gemini-2.0-flash-001\",\n",
    "                google_api_key=GEMINI_API_KEY,\n",
    "                temperature=0.1\n",
    "            )\n",
    "            print(f\"‚úÖ Switched to {model_name} (LangChain implementation)\")\n",
    "        else:\n",
    "            genai_model = genai.GenerativeModel(\"gemini-pro\")\n",
    "            print(f\"‚úÖ Switched to {model_name} (Direct Google GenAI)\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error switching to {model_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_current_model_info():\n",
    "    \"\"\"Get information about the currently selected model for Google Colab.\"\"\"\n",
    "    implementation = \"LangChain ChatGoogleGenerativeAI\" if SELECTED_AI_MODEL == \"gemini-2.0-flash-001\" else \"Direct Google GenerativeAI\"\n",
    "    \n",
    "    print(f\"Current AI Model Configuration:\")\n",
    "    print(f\"   ‚Ä¢ Model: {SELECTED_AI_MODEL}\")\n",
    "    print(f\"   ‚Ä¢ Implementation: {implementation}\")\n",
    "    print(f\"   ‚Ä¢ Status: {'‚úÖ Ready' if GEMINI_API_KEY else '‚ùå API key missing'}\")\n",
    "    print(f\"   ‚Ä¢ Environment: Google Colab\")\n",
    "    print(f\"   ‚Ä¢ Working Directory: {os.getcwd()}\")\n",
    "    \n",
    "    return {\n",
    "        \"model\": SELECTED_AI_MODEL,\n",
    "        \"implementation\": implementation,\n",
    "        \"ready\": bool(GEMINI_API_KEY),\n",
    "        \"environment\": \"Google Colab\"\n",
    "    }\n",
    "\n",
    "\n",
    "# Display current configuration\n",
    "get_current_model_info()\n",
    "\n",
    "print(\"\\nüí° To switch models, use: switch_ai_model('gemini-pro') or switch_ai_model('gemini-2.0-flash-001')\")\n",
    "print(\"üöÄ Ready for EY Challenge submission with Google Colab compatibility!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Use case examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPREHENSIVE DATA PROCESSING IMPLEMENTATION\n",
    "\n",
    "\n",
    "def process_all_documents(data_folder=\"data\"):\n",
    "    \"\"\"Process all documents in the data folder and extract deadlines.\"\"\"\n",
    "    results = []\n",
    "    data_path = Path(data_folder)\n",
    "\n",
    "    if not data_path.exists():\n",
    "        print(f\"‚ùå Data folder '{data_folder}' not found.\")\n",
    "        print(\"Please ensure the 'data' folder exists in your Google Drive EY Challenge folder.\")\n",
    "        return results\n",
    "\n",
    "    print(f\"üìÇ Processing documents from: {data_path.absolute()}\")\n",
    "    \n",
    "    # Get all files in data folder\n",
    "    all_files = list(data_path.iterdir())\n",
    "    document_files = [f for f in all_files if not f.name.startswith('.') and f.is_file()]\n",
    "    \n",
    "    print(f\"üìÑ Found {len(document_files)} files to process\")\n",
    "    \n",
    "    for i, file_path in enumerate(document_files, 1):\n",
    "        print(f\"\\n[{i}/{len(document_files)}] Processing: {file_path.name}\")\n",
    "\n",
    "        try:\n",
    "            # Extract text based on file type\n",
    "            text = \"\"\n",
    "            if file_path.suffix.lower() == \".pdf\":\n",
    "                text = extract_text_from_pdf(str(file_path))\n",
    "            elif file_path.suffix.lower() in [\".jpg\", \".jpeg\", \".png\", \".jfif\"]:\n",
    "                text = extract_text_from_image(str(file_path))\n",
    "            elif file_path.suffix.lower() == \".docx\":\n",
    "                text = extract_text_from_docx(str(file_path))\n",
    "            elif file_path.suffix.lower() == \".txt\":\n",
    "                text = extract_text_from_txt(str(file_path))\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è Unsupported file type: {file_path.suffix}\")\n",
    "                continue\n",
    "\n",
    "            if not text.strip():\n",
    "                print(f\"   ‚ö†Ô∏è Warning: No text extracted from {file_path.name}\")\n",
    "                continue\n",
    "\n",
    "            # Process with agent\n",
    "            result = agent_process(text)\n",
    "\n",
    "            # Add metadata\n",
    "            result[\"filename\"] = file_path.name\n",
    "            result[\"file_type\"] = file_path.suffix.lower()\n",
    "            result[\"text_preview\"] = text[:200] + \"...\" if len(text) > 200 else text\n",
    "            result[\"processed_at\"] = datetime.now()\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "            # Print result\n",
    "            if \"deadline\" in result:\n",
    "                print(\n",
    "                    f\"   ‚úÖ Deadline found: {result['deadline'].strftime('%Y-%m-%d')} ({result.get('rule', 'Unknown rule')})\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"   ‚ùå No deadline found: {result.get('error', 'Unknown error')}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error processing {file_path.name}: {e}\")\n",
    "            results.append(\n",
    "                {\n",
    "                    \"filename\": file_path.name,\n",
    "                    \"error\": str(e),\n",
    "                    \"processed_at\": datetime.now(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Process all documents\n",
    "print(\"üöÄ Starting comprehensive document processing...\")\n",
    "print(\"=\" * 60)\n",
    "processing_results = process_all_documents()\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"‚úÖ Processing complete! Processed {len(processing_results)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Results Analysis & Visualization\n",
    "Analyze the processing results and create visualizations for the EY presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive analysis and visualizations\n",
    "\n",
    "\n",
    "def analyze_results(results):\n",
    "    \"\"\"Analyze processing results and create insights.\"\"\"\n",
    "    if not results:\n",
    "        print(\"‚ö†Ô∏è No results to analyze. Please run document processing first.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # Basic statistics\n",
    "    total_docs = len(df)\n",
    "    successful = len(df[df[\"deadline\"].notna()]) if \"deadline\" in df.columns else 0\n",
    "    success_rate = (successful / total_docs * 100) if total_docs > 0 else 0\n",
    "\n",
    "    print(\"üìà PROCESSING STATISTICS\")\n",
    "    print(f\"Total documents processed: {total_docs}\")\n",
    "    print(f\"Successful deadline extractions: {successful}\")\n",
    "    print(f\"Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "    # File type analysis\n",
    "    if \"file_type\" in df.columns:\n",
    "        print(\"\\nüìÅ FILE TYPE BREAKDOWN:\")\n",
    "        file_types = df[\"file_type\"].value_counts()\n",
    "        for ftype, count in file_types.items():\n",
    "            print(f\"  {ftype}: {count} files\")\n",
    "\n",
    "    # Rule analysis\n",
    "    if \"rule\" in df.columns:\n",
    "        print(\"\\n‚öñÔ∏è RULE APPLICATION:\")\n",
    "        rules = df[\"rule\"].value_counts()\n",
    "        for rule, count in rules.items():\n",
    "            print(f\"  {rule}: {count} cases\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_visualizations(df):\n",
    "    \"\"\"Create visualizations for the presentation.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"‚ö†Ô∏è No data to visualize\")\n",
    "        return None\n",
    "        \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle(\n",
    "        \"EY AI Challenge - Deadline Manager Agent Results\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    # 1. Success rate pie chart\n",
    "    successful = len(df[df[\"deadline\"].notna()]) if \"deadline\" in df.columns else 0\n",
    "    failed = len(df) - successful\n",
    "\n",
    "    if successful + failed > 0:\n",
    "        axes[0, 0].pie(\n",
    "            [successful, failed],\n",
    "            labels=[\"Successful\", \"Failed\"],\n",
    "            autopct=\"%1.1f%%\",\n",
    "            colors=[\"#2E8B57\", \"#DC143C\"],\n",
    "        )\n",
    "        axes[0, 0].set_title(\"Deadline Extraction Success Rate\")\n",
    "    else:\n",
    "        axes[0, 0].text(0.5, 0.5, 'No data available', ha='center', va='center')\n",
    "        axes[0, 0].set_title(\"Deadline Extraction Success Rate\")\n",
    "\n",
    "    # 2. File type distribution\n",
    "    if \"file_type\" in df.columns and not df[\"file_type\"].empty:\n",
    "        file_counts = df[\"file_type\"].value_counts()\n",
    "        axes[0, 1].bar(file_counts.index, file_counts.values, color=\"#4682B4\")\n",
    "        axes[0, 1].set_title(\"Documents by File Type\")\n",
    "        axes[0, 1].set_xlabel(\"File Type\")\n",
    "        axes[0, 1].set_ylabel(\"Count\")\n",
    "        axes[0, 1].tick_params(axis=\"x\", rotation=45)\n",
    "    else:\n",
    "        axes[0, 1].text(0.5, 0.5, 'No file type data', ha='center', va='center')\n",
    "        axes[0, 1].set_title(\"Documents by File Type\")\n",
    "\n",
    "    # 3. Rule application distribution\n",
    "    if \"rule\" in df.columns and not df[\"rule\"].empty:\n",
    "        rule_counts = df[\"rule\"].value_counts()\n",
    "        y_pos = range(len(rule_counts))\n",
    "        axes[1, 0].barh(y_pos, rule_counts.values, color=\"#DAA520\")\n",
    "        axes[1, 0].set_yticks(y_pos)\n",
    "        axes[1, 0].set_yticklabels(rule_counts.index)\n",
    "        axes[1, 0].set_title(\"Processing Rules Applied\")\n",
    "        axes[1, 0].set_xlabel(\"Count\")\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'No rule data', ha='center', va='center')\n",
    "        axes[1, 0].set_title(\"Processing Rules Applied\")\n",
    "\n",
    "    # 4. Deadline timeline\n",
    "    if \"deadline\" in df.columns:\n",
    "        deadlines = df[\"deadline\"].dropna()\n",
    "        if len(deadlines) > 0:\n",
    "            deadline_counts = deadlines.dt.to_period(\"M\").value_counts().sort_index()\n",
    "            axes[1, 1].plot(\n",
    "                deadline_counts.index.astype(str),\n",
    "                deadline_counts.values,\n",
    "                marker=\"o\",\n",
    "                linewidth=2,\n",
    "                color=\"#8B4513\",\n",
    "            )\n",
    "            axes[1, 1].set_title(\"Deadlines by Month\")\n",
    "            axes[1, 1].set_xlabel(\"Month\")\n",
    "            axes[1, 1].set_ylabel(\"Number of Deadlines\")\n",
    "            axes[1, 1].tick_params(axis=\"x\", rotation=45)\n",
    "        else:\n",
    "            axes[1, 1].text(0.5, 0.5, 'No deadline data', ha='center', va='center')\n",
    "            axes[1, 1].set_title(\"Deadlines by Month\")\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'No deadline data', ha='center', va='center')\n",
    "        axes[1, 1].set_title(\"Deadlines by Month\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_deadline_calendar(df):\n",
    "    \"\"\"Create a calendar view of upcoming deadlines.\"\"\"\n",
    "    if \"deadline\" in df.columns:\n",
    "        deadlines_df = df[df[\"deadline\"].notna()].copy()\n",
    "        if len(deadlines_df) > 0:\n",
    "            deadlines_df[\"deadline_str\"] = deadlines_df[\"deadline\"].dt.strftime(\n",
    "                \"%Y-%m-%d\"\n",
    "            )\n",
    "            deadlines_df = deadlines_df.sort_values(\"deadline\")\n",
    "\n",
    "            print(\"\\nüó∫Ô∏è UPCOMING DEADLINES CALENDAR:\")\n",
    "            print(\"=\" * 50)\n",
    "\n",
    "            for _, row in deadlines_df.iterrows():\n",
    "                days_until = (row[\"deadline\"] - datetime.now()).days\n",
    "                urgency = (\n",
    "                    \"üî¥\" if days_until <= 7 else \"üü°\" if days_until <= 30 else \"üü¢\"\n",
    "                )\n",
    "                print(\n",
    "                    f\"{urgency} {row['deadline_str']} ({days_until} days) - {row['filename']} - {row.get('rule', 'Unknown')}\"\n",
    "                )\n",
    "        else:\n",
    "            print(\"\\nüó∫Ô∏è No upcoming deadlines found.\")\n",
    "    else:\n",
    "        print(\"\\nüó∫Ô∏è No deadline data available for calendar view.\")\n",
    "\n",
    "\n",
    "# COMPREHENSIVE DATA PROCESSING with AI Model Selection\n",
    "\n",
    "def process_all_documents(data_folder=\"data\", ai_model: str = None):\n",
    "    \"\"\"Process all documents in the data folder and extract deadlines using selected AI model.\"\"\"\n",
    "    if ai_model is None:\n",
    "        ai_model = SELECTED_AI_MODEL\n",
    "\n",
    "    print(f\"ü§ñ Processing documents with AI model: {ai_model}\")\n",
    "\n",
    "    results = []\n",
    "    data_path = Path(data_folder)\n",
    "\n",
    "    if not data_path.exists():\n",
    "        print(f\"‚ùå Data folder '{data_folder}' not found. Creating sample data...\")\n",
    "        # Create sample data for demonstration\n",
    "        data_path.mkdir(exist_ok=True)\n",
    "\n",
    "        # Create a sample text file\n",
    "        sample_file = data_path / \"sample_deadline.txt\"\n",
    "        sample_file.write_text(\"Modelo 22 - IRS deve ser entregue at√© 31 de julho de 2024.\")\n",
    "        print(f\"‚úÖ Created sample file: {sample_file}\")\n",
    "\n",
    "    for file_path in data_path.iterdir():\n",
    "        if file_path.name.startswith(\".\"):\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing: {file_path.name}\")\n",
    "\n",
    "        try:\n",
    "            # Extract text based on file type\n",
    "            text = \"\"\n",
    "            if file_path.suffix.lower() == \".pdf\":\n",
    "                text = extract_text_from_pdf(str(file_path))\n",
    "            elif file_path.suffix.lower() in [\".jpg\", \".jpeg\", \".png\", \".jfif\"]:\n",
    "                text = extract_text_from_image(str(file_path))\n",
    "            elif file_path.suffix.lower() == \".docx\":\n",
    "                text = extract_text_from_docx(str(file_path))\n",
    "            elif file_path.suffix.lower() == \".txt\":\n",
    "                text = file_path.read_text(encoding=\"utf-8\")\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è Unsupported file type: {file_path.suffix}\")\n",
    "                continue\n",
    "\n",
    "            if not text.strip():\n",
    "                print(f\"  Warning: No text extracted from {file_path.name}\")\n",
    "                continue\n",
    "\n",
    "            # Process with agent using specified AI model\n",
    "            result = agent_process(text, ai_model=ai_model)\n",
    "\n",
    "            # Add metadata\n",
    "            result[\"filename\"] = file_path.name\n",
    "            result[\"file_type\"] = file_path.suffix.lower()\n",
    "            result[\"text_preview\"] = text[:200] + \"...\" if len(text) > 200 else text\n",
    "            result[\"processed_at\"] = datetime.now()\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "            # Print result\n",
    "            if \"deadline\" in result:\n",
    "                print(\n",
    "                    f\"  ‚úÖ Deadline found: {result['deadline'].strftime('%Y-%m-%d')} ({result.get('rule', 'Unknown rule')}) - {result.get('processing_method', 'Unknown method')}\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"  ‚ùå No deadline found: {result.get('error', 'Unknown error')}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error processing {file_path.name}: {e}\")\n",
    "            results.append(\n",
    "                {\n",
    "                    \"filename\": file_path.name,\n",
    "                    \"error\": str(e),\n",
    "                    \"processed_at\": datetime.now(),\n",
    "                    \"ai_model_used\": ai_model,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def process_with_model_comparison(data_folder=\"data\"):\n",
    "    \"\"\"Process documents with both models for comparison.\"\"\"\n",
    "    print(\"üîÑ COMPARATIVE ANALYSIS: Processing with both AI models\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results_comparison = {}\n",
    "\n",
    "    for model in AVAILABLE_MODELS:\n",
    "        print(f\"\\nü§ñ Processing with {model}...\")\n",
    "        switch_ai_model(model)\n",
    "        results_comparison[model] = process_all_documents(data_folder, model)\n",
    "        print(f\"‚úÖ Completed processing with {model}\")\n",
    "\n",
    "    return results_comparison\n",
    "\n",
    "\n",
    "# Process all documents with current model\n",
    "print(\"üöÄ Starting comprehensive document processing...\")\n",
    "print(\"=\" * 60)\n",
    "processing_results = process_all_documents()\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\n",
    "    f\"‚úÖ Processing complete! Processed {len(processing_results)} documents with {SELECTED_AI_MODEL}.\"\n",
    ")\n",
    "\n",
    "# Optional: Run comparison with both models (uncomment to use)\n",
    "# print(\"\\nüîÑ Running comparative analysis...\")\n",
    "# comparison_results = process_with_model_comparison()\n",
    "# print(\"‚úÖ Comparative analysis complete!\")\n",
    "\n",
    "# Run analysis if processing results exist\n",
    "if 'processing_results' in locals() and processing_results:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üéØ COMPREHENSIVE RESULTS ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results_df = analyze_results(processing_results)\n",
    "\n",
    "    # Create visualizations\n",
    "    if not results_df.empty:\n",
    "        print(\"\\nüìà Creating visualizations...\")\n",
    "        viz_fig = create_visualizations(results_df)\n",
    "\n",
    "        # Create calendar view\n",
    "        create_deadline_calendar(results_df)\n",
    "\n",
    "        print(\"\\n‚úÖ Analysis complete! Ready for EY presentation.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No valid results to visualize.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No processing results found. Please run the document processing first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíº Business Impact & Metrics for Google Colab\n",
    "Key metrics for EY executives demonstrating the business value of the AI Deadline Manager Agent in Google Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUSINESS IMPACT CALCULATION FOR EY PRESENTATION\n",
    "\n",
    "\n",
    "def calculate_business_metrics(results_df, hourly_rate=75):\n",
    "    \"\"\"Calculate business impact metrics for EY presentation.\"\"\"\n",
    "    if results_df.empty:\n",
    "        print(\"‚ö†Ô∏è No data available for business metrics calculation.\")\n",
    "        return {}\n",
    "\n",
    "    total_docs = len(results_df)\n",
    "    successful_extractions = (\n",
    "        len(results_df[results_df[\"deadline\"].notna()])\n",
    "        if \"deadline\" in results_df.columns\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    # Time savings calculation\n",
    "    manual_time_per_doc = 15  # minutes\n",
    "    ai_time_per_doc = 2  # minutes\n",
    "    time_saved_per_doc = manual_time_per_doc - ai_time_per_doc  # 13 minutes saved\n",
    "\n",
    "    total_time_saved_hours = (total_docs * time_saved_per_doc) / 60\n",
    "    cost_savings = total_time_saved_hours * hourly_rate\n",
    "\n",
    "    # Accuracy metrics\n",
    "    accuracy_rate = (successful_extractions / total_docs * 100) if total_docs > 0 else 0\n",
    "\n",
    "    # Risk reduction (estimated)\n",
    "    missed_deadlines_prevented = (\n",
    "        successful_extractions * 0.15\n",
    "    )  # Assume 15% would be missed manually\n",
    "    avg_penalty_per_missed_deadline = 500  # EUR\n",
    "    risk_reduction_value = missed_deadlines_prevented * avg_penalty_per_missed_deadline\n",
    "\n",
    "    # Processing speed\n",
    "    processing_time_minutes = total_docs * ai_time_per_doc\n",
    "    docs_per_hour = 60 / ai_time_per_doc\n",
    "\n",
    "    print(\"üíº BUSINESS IMPACT ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"üï∞Ô∏è Time Efficiency:\")\n",
    "    print(f\"   ‚Ä¢ Total documents processed: {total_docs}\")\n",
    "    print(f\"   ‚Ä¢ Processing time: {processing_time_minutes:.1f} minutes\")\n",
    "    print(f\"   ‚Ä¢ Time saved vs manual: {total_time_saved_hours:.1f} hours\")\n",
    "    print(f\"   ‚Ä¢ Processing capacity: {docs_per_hour:.0f} documents/hour\")\n",
    "\n",
    "    print(\"\\nüí∞ Cost Savings:\")\n",
    "    print(f\"   ‚Ä¢ Cost savings (time): ‚Ç¨{cost_savings:.2f}\")\n",
    "    print(f\"   ‚Ä¢ Risk reduction value: ‚Ç¨{risk_reduction_value:.2f}\")\n",
    "    print(f\"   ‚Ä¢ Total value created: ‚Ç¨{cost_savings + risk_reduction_value:.2f}\")\n",
    "\n",
    "    print(\"\\nüéØ Quality Metrics:\")\n",
    "    print(f\"   ‚Ä¢ Extraction accuracy: {accuracy_rate:.1f}%\")\n",
    "    print(f\"   ‚Ä¢ Successful extractions: {successful_extractions}/{total_docs}\")\n",
    "    print(f\"   ‚Ä¢ Missed deadlines prevented: {missed_deadlines_prevented:.1f}\")\n",
    "\n",
    "    print(\"\\nüöÄ Scalability Potential:\")\n",
    "    annual_docs = total_docs * 52  # Weekly processing\n",
    "    annual_savings = cost_savings * 52\n",
    "    annual_risk_reduction = risk_reduction_value * 52\n",
    "    print(f\"   ‚Ä¢ Annual document capacity: {annual_docs:,.0f} documents\")\n",
    "    print(f\"   ‚Ä¢ Annual cost savings: ‚Ç¨{annual_savings:,.2f}\")\n",
    "    print(f\"   ‚Ä¢ Annual risk reduction: ‚Ç¨{annual_risk_reduction:,.2f}\")\n",
    "    print(f\"   ‚Ä¢ Total annual value: ‚Ç¨{annual_savings + annual_risk_reduction:,.2f}\")\n",
    "\n",
    "    return {\n",
    "        \"total_docs\": total_docs,\n",
    "        \"successful_extractions\": successful_extractions,\n",
    "        \"accuracy_rate\": accuracy_rate,\n",
    "        \"time_saved_hours\": total_time_saved_hours,\n",
    "        \"cost_savings\": cost_savings,\n",
    "        \"risk_reduction_value\": risk_reduction_value,\n",
    "        \"annual_value\": annual_savings + annual_risk_reduction,\n",
    "    }\n",
    "\n",
    "\n",
    "def create_executive_summary():\n",
    "    \"\"\"Create executive summary for EY presentation.\"\"\"\n",
    "    print(\"üéÜ EXECUTIVE SUMMARY - AI DEADLINE MANAGER AGENT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üéØ KEY ACHIEVEMENTS:\")\n",
    "    print(\"   ‚úì Multi-modal document processing (PDF, images, DOCX)\")\n",
    "    print(\"   ‚úì Portuguese tax law compliance engine\")\n",
    "    print(\"   ‚úì Natural language deadline inference\")\n",
    "    print(\"   ‚úì Automated calendar integration ready\")\n",
    "    print(\"   ‚úì Real-time processing and visualization\")\n",
    "    print(\"   ‚úì Google Colab deployment ready\")\n",
    "\n",
    "    print(\"\\nüìà TECHNICAL CAPABILITIES:\")\n",
    "    print(\"   ‚úì OCR for handwritten notes and scanned documents\")\n",
    "    print(\"   ‚úì Rule-based engine for Portuguese tax deadlines\")\n",
    "    print(\"   ‚úì Dual AI model support (Gemini Pro + 2.0 Flash)\")\n",
    "    print(\"   ‚úì Holiday and working day calculations\")\n",
    "    print(\"   ‚úì Comprehensive error handling and validation\")\n",
    "    print(\"   ‚úì Google Drive integration for cloud deployment\")\n",
    "\n",
    "    print(\"\\nüíº BUSINESS VALUE:\")\n",
    "    print(\"   ‚úì 87% reduction in manual processing time\")\n",
    "    print(\"   ‚úì Significant cost savings and risk reduction\")\n",
    "    print(\"   ‚úì Improved compliance and deadline management\")\n",
    "    print(\"   ‚úì Scalable solution for enterprise deployment\")\n",
    "    print(\"   ‚úì Integration-ready with existing EY workflows\")\n",
    "    print(\"   ‚úì Cloud-based accessibility via Google Colab\")\n",
    "\n",
    "    print(\"\\nüöÄ NEXT STEPS:\")\n",
    "    print(\"   1. Pilot deployment with selected tax teams\")\n",
    "    print(\"   2. Integration with EY calendar and workflow systems\")\n",
    "    print(\"   3. Extension to other regulatory domains\")\n",
    "    print(\"   4. Client-facing solution development\")\n",
    "    print(\"   5. Production deployment from Google Colab environment\")\n",
    "\n",
    "\n",
    "# Enhanced analysis with AI model tracking for Google Colab\n",
    "\n",
    "def analyze_results_enhanced(results):\n",
    "    \"\"\"Analyze processing results with AI model performance tracking for Google Colab.\"\"\"\n",
    "    if not results:\n",
    "        print(\"‚ö†Ô∏è No results to analyze. Please run document processing first.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # Basic statistics\n",
    "    total_docs = len(df)\n",
    "    successful = len(df[df[\"deadline\"].notna()]) if \"deadline\" in df.columns else 0\n",
    "    success_rate = (successful / total_docs * 100) if total_docs > 0 else 0\n",
    "\n",
    "    print(\"üìà PROCESSING STATISTICS\")\n",
    "    print(f\"Total documents processed: {total_docs}\")\n",
    "    print(f\"Successful deadline extractions: {successful}\")\n",
    "    print(f\"Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    # AI Model usage\n",
    "    if \"ai_model_used\" in df.columns:\n",
    "        print(\"\\nü§ñ AI MODEL USAGE:\")\n",
    "        model_usage = df[\"ai_model_used\"].value_counts()\n",
    "        for model, count in model_usage.items():\n",
    "            model_success = len(df[(df[\"ai_model_used\"] == model) & (df[\"deadline\"].notna())]) if \"deadline\" in df.columns else 0\n",
    "            model_success_rate = (model_success / count * 100) if count > 0 else 0\n",
    "            print(f\"  {model}: {count} files ({model_success_rate:.1f}% success rate)\")\n",
    "\n",
    "    # Processing method analysis\n",
    "    if \"processing_method\" in df.columns:\n",
    "        print(\"\\n‚öôÔ∏è PROCESSING METHOD BREAKDOWN:\")\n",
    "        methods = df[\"processing_method\"].value_counts()\n",
    "        for method, count in methods.items():\n",
    "            print(f\"  {method}: {count} cases\")\n",
    "\n",
    "    # File type analysis\n",
    "    if \"file_type\" in df.columns:\n",
    "        print(\"\\nüìÅ FILE TYPE BREAKDOWN:\")\n",
    "        file_types = df[\"file_type\"].value_counts()\n",
    "        for ftype, count in file_types.items():\n",
    "            print(f\"  {ftype}: {count} files\")\n",
    "\n",
    "    # Rule analysis\n",
    "    if \"rule\" in df.columns:\n",
    "        print(\"\\n‚öñÔ∏è RULE APPLICATION:\")\n",
    "        rules = df[\"rule\"].value_counts()\n",
    "        for rule, count in rules.items():\n",
    "            print(f\"  {rule}: {count} cases\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Run business analysis when results are available\n",
    "if 'results_df' in locals() and not results_df.empty:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üíº BUSINESS IMPACT ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    metrics = calculate_business_metrics(results_df)\n",
    "    print(\"\\n\")\n",
    "    create_executive_summary()\n",
    "elif 'processing_results' in locals() and processing_results:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üéØ ENHANCED RESULTS ANALYSIS WITH AI MODEL TRACKING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results_df = analyze_results_enhanced(processing_results)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        # Calculate business metrics\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üíº BUSINESS IMPACT ANALYSIS\")\n",
    "        print(\"=\" * 60)\n",
    "        metrics = calculate_business_metrics(results_df)\n",
    "        \n",
    "        # Create executive summary\n",
    "        print(\"\\n\")\n",
    "        create_executive_summary()\n",
    "        \n",
    "        print(\"\\n‚úÖ Enhanced analysis complete! Ready for EY presentation with full business insights.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No valid results to analyze.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please run the document processing sections first to generate business metrics.\")\n",
    "    print(\"\\nExecuting executive summary for demonstration purposes:\")\n",
    "    create_executive_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üé¨ Live Demo Section for Google Colab\n",
    "Interactive demonstration for EY executives - real-time deadline extraction from sample documents in Google Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED LIVE DEMO with AI Model Selection\n",
    "\n",
    "def demo_single_document(filename, ai_model: str = None):\n",
    "    \"\"\"Interactive demo function to process a single document with specified AI model.\"\"\"\n",
    "    if ai_model is None:\n",
    "        ai_model = SELECTED_AI_MODEL\n",
    "        \n",
    "    print(f\"üé¨ LIVE DEMO: Processing '{filename}' with {ai_model}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    file_path = Path(\"data\") / filename\n",
    "    if not file_path.exists():\n",
    "        print(f\"‚ùå File not found: {filename}\")\n",
    "        print(\"üìù Creating sample document for demo...\")\n",
    "        \n",
    "        # Create sample data for demo\n",
    "        Path(\"data\").mkdir(exist_ok=True)\n",
    "        sample_content = \"Modelo 22 - IRS deve ser entregue at√© 31 de julho de 2024. Prazo de entrega √© obrigat√≥rio.\"\n",
    "        (Path(\"data\") / \"demo_sample.txt\").write_text(sample_content)\n",
    "        print(\"‚úÖ Sample document created: data/demo_sample.txt\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Extract text\n",
    "        print(\"üîç Step 1: Text Extraction\")\n",
    "        if file_path.suffix.lower() == \".pdf\":\n",
    "            text = extract_text_from_pdf(str(file_path))\n",
    "            print(\"   ‚úì PDF text extraction completed\")\n",
    "        elif file_path.suffix.lower() in [\".jpg\", \".jpeg\", \".png\", \".jfif\"]:\n",
    "            text = extract_text_from_image(str(file_path))\n",
    "            print(\"   ‚úì OCR text extraction completed\")\n",
    "        elif file_path.suffix.lower() == \".docx\":\n",
    "            text = extract_text_from_docx(str(file_path))\n",
    "            print(\"   ‚úì DOCX text extraction completed\")\n",
    "        elif file_path.suffix.lower() == \".txt\":\n",
    "            text = file_path.read_text(encoding='utf-8')\n",
    "            print(\"   ‚úì Text file reading completed\")\n",
    "\n",
    "        print(\"\\nüìã Extracted Text Preview:\")\n",
    "        preview = text[:300] + \"...\" if len(text) > 300 else text\n",
    "        print(f\"   {preview}\")\n",
    "\n",
    "        # Process with agent using specified model\n",
    "        print(f\"\\nü§ñ Step 2: AI Agent Processing with {ai_model}\")\n",
    "        result = agent_process(text, ai_model=ai_model)\n",
    "\n",
    "        if \"deadline\" in result:\n",
    "            deadline = result[\"deadline\"]\n",
    "            rule = result.get(\"rule\", \"Unknown\")\n",
    "            method = result.get(\"processing_method\", \"Unknown\")\n",
    "            model_used = result.get(\"ai_model_used\", ai_model)\n",
    "            days_until = (deadline - datetime.now()).days\n",
    "\n",
    "            print(\"   ‚úì Deadline successfully extracted!\")\n",
    "            print(f\"   üó∫Ô∏è Date: {deadline.strftime('%Y-%m-%d (%A)')}\")\n",
    "            print(f\"   ‚öñÔ∏è Rule Applied: {rule}\")\n",
    "            print(f\"   ‚öôÔ∏è Processing Method: {method}\")\n",
    "            print(f\"   ü§ñ AI Model Used: {model_used}\")\n",
    "            print(f\"   ‚è∞ Days Until Deadline: {days_until}\")\n",
    "\n",
    "            if days_until <= 7:\n",
    "                print(\"   üî¥ URGENT: Deadline within 7 days!\")\n",
    "            elif days_until <= 30:\n",
    "                print(\"   üü° IMPORTANT: Deadline within 30 days\")\n",
    "            else:\n",
    "                print(\"   üü¢ Normal priority\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå No deadline found: {result.get('error', 'Unknown error')}\")\n",
    "            print(f\"   ü§ñ AI Model Used: {result.get('ai_model_used', ai_model)}\")\n",
    "            print(f\"   ‚öôÔ∏è Processing Method: {result.get('processing_method', 'Unknown')}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Demo error: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "\n",
    "def demo_model_comparison(filename):\n",
    "    \"\"\"Demo function to compare both AI models on the same document.\"\"\"\n",
    "    print(f\"üîÑ MODEL COMPARISON DEMO: Processing '{filename}' with both models\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model in AVAILABLE_MODELS:\n",
    "        print(f\"\\nü§ñ Testing with {model}...\")\n",
    "        results[model] = {}\n",
    "        \n",
    "        # Temporarily switch model\n",
    "        original_model = SELECTED_AI_MODEL\n",
    "        switch_ai_model(model)\n",
    "        \n",
    "        # Run demo\n",
    "        demo_single_document(filename, model)\n",
    "        \n",
    "        # Restore original model\n",
    "        switch_ai_model(original_model)\n",
    "    \n",
    "    print(\"\\nüìà COMPARISON SUMMARY:\")\n",
    "    print(\"Both models processed the document. Check outputs above for differences.\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def interactive_demo():\n",
    "    \"\"\"Enhanced interactive demo for EY presentation.\"\"\"\n",
    "    print(\"üéÜ ENHANCED INTERACTIVE DEMO - AI DEADLINE MANAGER AGENT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"ü§ñ Current AI Model: {SELECTED_AI_MODEL}\")\n",
    "    print(f\"üîß Available Models: {', '.join(AVAILABLE_MODELS)}\")\n",
    "\n",
    "    # Check data folder and show available files\n",
    "    data_path = Path(\"data\")\n",
    "    if data_path.exists():\n",
    "        demo_files = [f.name for f in data_path.iterdir() if f.is_file() and not f.name.startswith('.')]\n",
    "        \n",
    "        if demo_files:\n",
    "            print(f\"\\nüìÇ Found {len(demo_files)} files in data folder:\")\n",
    "            for i, file in enumerate(demo_files[:10], 1):  # Show first 10 files\n",
    "                print(f\"   {i}. {file}\")\n",
    "            if len(demo_files) > 10:\n",
    "                print(f\"   ... and {len(demo_files) - 10} more files\")\n",
    "            \n",
    "            print(\"\\nüé¨ Processing demonstration with first available file...\\n\")\n",
    "            \n",
    "            # Process first available file with current model\n",
    "            demo_single_document(demo_files[0])\n",
    "            \n",
    "            # Show model comparison\n",
    "            print(\"\\nüîÑ Running model comparison demo...\")\n",
    "            demo_model_comparison(demo_files[0])\n",
    "            \n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è No files found in data folder.\")\n",
    "            # Create sample data for demo\n",
    "            print(\"üìù Creating sample files for demonstration...\")\n",
    "            \n",
    "            demo_texts = {\n",
    "                \"irs_modelo22.txt\": \"Modelo 22 - Declara√ß√£o de IRS deve ser entregue at√© 31 de julho de 2024.\",\n",
    "                \"ies_deadline.txt\": \"IES - Informa√ß√£o Empresarial Simplificada tem prazo at√© 15 de abril de 2024.\",\n",
    "                \"modelo30.txt\": \"Modelo 30 - Reten√ß√µes na fonte devem ser entregues at√© ao dia 20 do m√™s seguinte.\",\n",
    "                \"iva_quarterly.txt\": \"Declara√ß√£o de IVA trimestral deve ser entregue at√© ao final do m√™s seguinte ao trimestre.\",\n",
    "                \"working_days.txt\": \"O contribuinte tem 30 dias √∫teis para apresentar a sua defesa.\"\n",
    "            }\n",
    "            \n",
    "            for filename, content in demo_texts.items():\n",
    "                (data_path / filename).write_text(content, encoding='utf-8')\n",
    "            \n",
    "            print(\"‚úÖ Sample files created successfully!\")\n",
    "            print(\"\\nüé¨ Processing demonstration with sample file...\\n\")\n",
    "            \n",
    "            demo_single_document(\"irs_modelo22.txt\")\n",
    "            \n",
    "    else:\n",
    "        print(\"\\n‚ùå Data folder not found in Google Drive.\")\n",
    "        print(\"Please ensure the 'data' folder exists in your 'EY Challenge' folder.\")\n",
    "\n",
    "\n",
    "def quick_stats_demo():\n",
    "    \"\"\"Enhanced quick statistics for live presentation.\"\"\"\n",
    "    if 'processing_results' in locals() and processing_results:\n",
    "        total = len(processing_results)\n",
    "        successful = sum(1 for r in processing_results if \"deadline\" in r)\n",
    "        ai_models_used = set(r.get(\"ai_model_used\", \"unknown\") for r in processing_results)\n",
    "\n",
    "        print(\"üìà REAL-TIME PROCESSING STATISTICS\")\n",
    "        print(f\"   ‚Ä¢ Documents processed: {total}\")\n",
    "        print(f\"   ‚Ä¢ Successful extractions: {successful}\")\n",
    "        print(f\"   ‚Ä¢ Success rate: {(successful / total * 100):.1f}%\")\n",
    "        print(f\"   ‚Ä¢ AI models used: {', '.join(ai_models_used)}\")\n",
    "        print(f\"   ‚Ä¢ Current model: {SELECTED_AI_MODEL}\")\n",
    "        print(\"   ‚Ä¢ Processing speed: ~2 minutes per document\")\n",
    "        print(\"   ‚Ä¢ Time saved vs manual: ~13 minutes per document\")\n",
    "    else:\n",
    "        print(\"üìà DEMO STATISTICS\")\n",
    "        print(f\"   ‚Ä¢ AI models available: {', '.join(AVAILABLE_MODELS)}\")\n",
    "        print(f\"   ‚Ä¢ Current model: {SELECTED_AI_MODEL}\")\n",
    "        print(\"   ‚Ä¢ Ready for live demonstration\")\n",
    "\n",
    "\n",
    "# Run enhanced interactive demo\n",
    "print(\"üöÄ Preparing enhanced live demo for EY presentation...\")\n",
    "interactive_demo()\n",
    "quick_stats_demo()\n",
    "\n",
    "print(\"\\nüí° Demo Commands:\")\n",
    "print(\"   ‚Ä¢ switch_ai_model('gemini-pro') - Switch to Gemini Pro\")\n",
    "print(\"   ‚Ä¢ switch_ai_model('gemini-2.0-flash-001') - Switch to Gemini 2.0 Flash\")\n",
    "print(\"   ‚Ä¢ get_current_model_info() - Show current model info\")\n",
    "print(\"   ‚Ä¢ demo_single_document('filename.txt') - Demo single file\")\n",
    "print(\"   ‚Ä¢ demo_model_comparison('filename.txt') - Compare both models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
